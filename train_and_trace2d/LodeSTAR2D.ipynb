{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import deeplay as dl\n",
    "import deeptrack as dt\n",
    "import h5py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from matplotlib.widgets import Slider\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # First block of three 3x3 conv layers with 32 filters each\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second block of eight 3x3 conv layers with 32 filters each\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv11 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Final 1x1 conv layer with 3 filters\n",
    "        self.final_conv = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply the first three convolutional layers with ReLU\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        # Apply max pooling\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Apply the next eight convolutional layers with ReLU\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = F.relu(self.conv11(x))\n",
    "        \n",
    "        # Apply the final convolutional layer without activation\n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_standard(image_size: int=64, noise_value: float=1e-4, position: tuple[int]=(0,0), minv: float=0, maxv: float=1) -> tuple:\n",
    "\n",
    "    \"\"\"Generates an image of a particle an its position\"\"\"\n",
    "    \n",
    "    particle=dt.MieSphere(position=(position[0]+image_size//2,position[1]+image_size//2), radius=7e-8, refractive_index=1.4, z=0, position_objective=np.array([0, 0, 0]))\n",
    "    args=dt.Arguments(hccoeff=lambda: np.random.uniform(-100,100))\n",
    "    pupil=dt.HorizontalComa(coefficient=args.hccoeff)\n",
    "    optics=dt.Brightfield(NA=1.0,working_distance=.2e-3,aberration=pupil,wavelength=660e-9,resolution=.15e-6,magnification=1,output_region=(0,0,image_size,image_size),return_field=False,illumination_angle=np.pi) \n",
    "    \n",
    "    def phase_adder(ph):\n",
    "        def inner(image):\n",
    "            image=image-1\n",
    "            image=image*np.exp(1j*ph)\n",
    "            image=image+1\n",
    "            return np.abs(image)\n",
    "        return inner\n",
    "    \n",
    "    phadd=dt.Lambda(phase_adder,ph=np.pi/2)\n",
    "    s0=optics(particle)\n",
    "    sample=s0>>phadd\n",
    "    sample=(sample>>dt.Gaussian(sigma=noise_value))\n",
    "    sample=(sample>>dt.NormalizeMinMax(minv, maxv))\n",
    "    im = sample.update()()\n",
    "    positions = im.get_property('position', get_one=False)\n",
    "    return im, positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle_dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"Base Particle_dataset from which all specialized data sets inherit. \\n\n",
    "    Contains necessary methods for torch, such as __len__ and __getitem__, \n",
    "    and some useful methods, such as __add__ and __setitem__\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.images=[]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, i: int):\n",
    "        return self.images[i]\n",
    "    \n",
    "    def __setitem__(self, i: int, value: any):\n",
    "        self.images[i] = value\n",
    "        \n",
    "    def __add__(self, data):\n",
    "\n",
    "        \"\"\"Combines two datasets into one, or adds images to an existing dataset\"\"\" \n",
    "\n",
    "        if 'dataset' in str(type(data)) or 'Matlab' in str(type(data)):\n",
    "            self.images += data.images\n",
    "        else:\n",
    "            self.images += [data]\n",
    "        return self\n",
    "    \n",
    "    def __neg__(self):\n",
    "        \n",
    "        \"\"\"Negates the pixel values in the image (inverts the image)\"\"\"\n",
    "\n",
    "        for i in range(len(self.images)):\n",
    "            self.images[i] = -self.images[i]\n",
    "        return self\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.images\n",
    "    \n",
    "    def delete(self, i: int):\n",
    "        self.images = [image for image, j in enumerate(self.images) if j != i]\n",
    "\n",
    "class Image_dataset(Particle_dataset):\n",
    "\n",
    "    \"\"\"Creates a Particle_dataset from all images (.png) contained in the folder path\n",
    "    Training the network on .png files is not recommended due to possible loss of information\n",
    "    during saving and loading from our method, but could work if care is taken.\"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.folder_path = path\n",
    "        self.extract_pngs(path)\n",
    "\n",
    "    def extract_pngs(self, folder_path):\n",
    "        self.image_paths = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(filepath):\n",
    "                self.image_paths.append(filepath)\n",
    "\n",
    "        transform = transforms.ToTensor()\n",
    "        for image_path in self.image_paths:\n",
    "            img = Image.open(image_path)\n",
    "            img_tensor = transform(img)\n",
    "            self.images.append(img_tensor)\n",
    "\n",
    "class Array_dataset(Particle_dataset):\n",
    "\n",
    "    \"\"\"Creates a Particle_dataset from all arrays (.npy) contained in the folder path\"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.folder_path = path\n",
    "        self.extract_arrays(path)\n",
    "\n",
    "    def extract_arrays(self, folder_path):\n",
    "        self.image_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path)]    \n",
    "        \n",
    "        for image_path in self.image_paths:\n",
    "            image = np.load(image_path).astype('float32')\n",
    "            img_tensor = torch.from_numpy(image).unsqueeze(0)\n",
    "            self.images.append(img_tensor)\n",
    "\n",
    "class Generative_dataset(Particle_dataset):\n",
    "\n",
    "    \"\"\"Generates a Particle_dataset from a default deeptrack generator if not otherwise specified. \\n\n",
    "    Invert=True will invert the image value \\n\n",
    "    If both inverted and non-inverted images are used for training, make sure to normalize\n",
    "    in some way to make the images comparable to each other\"\"\"\n",
    "\n",
    "    def __init__(self, num_samples=1, generator=generate_standard, invert=False, **kwargs):\n",
    "        super().__init__()\n",
    "        func = lambda x: (-1)**invert*x\n",
    "        for i in range(num_samples):\n",
    "            image, label = generator(**kwargs)\n",
    "            self.images.append(func(torch.tensor(image).float().permute(2, 0, 1)))\n",
    "\n",
    "class Matlab_dataset(Particle_dataset):\n",
    "\n",
    "    \"\"\"Create a Particle_dataset from a matlab file with images in range \"Range\". \\n\n",
    "    If Range is not specified, all images in the matlab file will be loaded which takes\n",
    "    a lot of space and time\"\"\"\n",
    "\n",
    "    def __init__(self, path: str=None, Range: tuple[int]=None):\n",
    "        super().__init__()\n",
    "        file = h5py.File(path)['Im_stack']\n",
    "        if Range!=None:\n",
    "            for i in range(*Range):\n",
    "                image = file[i]\n",
    "                img_tensor = torch.from_numpy(image).unsqueeze(0)\n",
    "                self.images.append(img_tensor.type(torch.float32))\n",
    "        else:\n",
    "            for image in file:\n",
    "                img_tensor = torch.from_numpy(image).unsqueeze(0)\n",
    "                self.images.append(img_tensor.type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.epochs = 2000\n",
    "        self.num_transforms = 8\n",
    "        self.batch_size = 1\n",
    "        self.learning_rate = 1e-4\n",
    "        self.position = (0,0)\n",
    "        self.num_samples = 1 # Does nothing atm\n",
    "        self.lower_b = None # Does nothing atm\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        self.Network = CNN().to(self.device)\n",
    "        self.lodestar = dl.LodeSTAR(model=self.Network, n_transforms=self.num_transforms)\n",
    "\n",
    "        self.dataset = Particle_dataset()\n",
    "        self.trainset = Particle_dataset()\n",
    "\n",
    "    def set_dataset(self, dataset: Particle_dataset) -> None:\n",
    "\n",
    "        \"\"\"Set trainer's regular data set (recommended method)\"\"\"\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def add_dataset(self, dataset: Particle_dataset) -> None:\n",
    "\n",
    "        \"\"\"Add a data set to trainer's regular data set (recommended method). \\n\n",
    "        Only works if data set has an __add__ method such as a Particle_dataset does\"\"\"\n",
    "\n",
    "        self.dataset += dataset\n",
    "\n",
    "    def generate_data(self, **kwargs) -> None:\n",
    "\n",
    "        \"\"\"Add generated images from deeptrack to the trainer's regular data set (not recommended)\"\"\"\n",
    "\n",
    "        generated_data = Generative_dataset(\n",
    "            image_size=kwargs.get('image_size', 64), \n",
    "            generator=kwargs.get('generator', generate_standard), \n",
    "            invert=kwargs.get('invert', False), \n",
    "            **kwargs\n",
    "            )\n",
    "        self.dataset += generated_data\n",
    "\n",
    "    def load_data(self, folder_path: str=os.path.join(os.getcwd(), r'\\Bilder'), mapp: str='static_removed_images', Dataset: Particle_dataset=Array_dataset, **kwargs) -> None:\n",
    "        \n",
    "        \"\"\"Add loaded images to the trainer's regular data set (not recommended)\"\"\"\n",
    "        \n",
    "        loaded_data = Dataset(folder_path, mapp, **kwargs)\n",
    "        self.dataset += loaded_data\n",
    "\n",
    "    def clear_data(self) -> None:\n",
    "\n",
    "        \"\"\"Clear trainer's regular data set\"\"\"\n",
    "\n",
    "        self.dataset = Particle_dataset()\n",
    "    \n",
    "    def clear_train_data(self) -> None:\n",
    "\n",
    "        \"\"\"Clear trainer's train set\"\"\"\n",
    "\n",
    "        self.trainset = Particle_dataset()\n",
    "\n",
    "    def add_train_data(self, i: int|list|torch.Tensor=None) -> None:\n",
    "\n",
    "        \"\"\"Add data to trainer's train set. \\n\n",
    "        If None the trainer will load the entire dataset as trainset.\"\"\"\n",
    "\n",
    "        if i==None:\n",
    "            self.trainset += self.dataset\n",
    "        elif type(i)==list:\n",
    "            for j in i:\n",
    "                self.trainset += self.dataset[j]\n",
    "        elif type(i)==int:\n",
    "            self.trainset += self.dataset[i]\n",
    "        elif 'torch' in str(type(i)):\n",
    "            self.trainset += i\n",
    "\n",
    "    def show_data(self, i: int|list=None) -> None:\n",
    "\n",
    "        \"\"\"Plots all data from trainer's regular data if indices are not inputted\"\"\"\n",
    "\n",
    "        if i==None:\n",
    "            for j in range(len(self.dataset)):\n",
    "                plt.title(j)\n",
    "                plt.imshow(self.dataset[j].squeeze(), cmap='gray')\n",
    "                plt.show()\n",
    "        elif type(i)==list:\n",
    "            for j in i:\n",
    "                plt.title(j)\n",
    "                plt.imshow(self.dataset[j].squeeze(), cmap='gray')\n",
    "                plt.show()\n",
    "        elif type(i)==int:\n",
    "            plt.title(i)\n",
    "            plt.imshow(self.dataset[i].squeeze(), cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "    def transform(self, image: torch.FloatTensor, mode: str='barebones') -> None:\n",
    "\n",
    "        \"\"\"Extension of the transform_data transform. \\n \n",
    "        If the mode='realistic' then random multiplication and noise will be applied to every transformation. \"\"\"\n",
    "\n",
    "        if mode=='realistic':\n",
    "            image *= np.random.uniform(0.1, 2)\n",
    "            image += np.random.normal(scale=np.random.rand()*1e-3, size=image.shape).astype('float32')\n",
    "            transforms, inverses = self.lodestar.transform_data(image)\n",
    "        if mode=='barebones':\n",
    "            transforms, inverses = self.lodestar.transform_data(image)\n",
    "        return transforms, inverses\n",
    "\n",
    "    def train(self, mode: str='barebones') -> None:\n",
    "\n",
    "        \"\"\"Starts training the network on the trainers train set\"\"\"\n",
    "\n",
    "        if len(self.trainset)==0:\n",
    "            print('No train data. Add train data with add_train_data.')\n",
    "            return\n",
    "        \n",
    "        dataloader = DataLoader(self.trainset, shuffle=True)\n",
    "        optimizer = Adam(params = self.Network.parameters(), lr=self.learning_rate)\n",
    "        self.losses = []\n",
    "\n",
    "        for i in range(1, self.epochs+1):\n",
    "            if i%100 == 0:\n",
    "                clear_output()\n",
    "                print(f'Epoch {i}/{self.epochs} \\t Loss: {loss:.3f}')\n",
    "                if i!=self.epochs-1:\n",
    "                    print('|'+'-'*(35*i//self.epochs) + ' '*(35-(35*i//self.epochs)) + '|')\n",
    "                else:\n",
    "                    print('|'+'-'*35+'|')\n",
    "\n",
    "            for image in dataloader:\n",
    "                image = image.to(self.device)\n",
    "                transforms, inverses = self.transform(image, mode=mode)\n",
    "                if i == 1:\n",
    "                    self.transforms_example = transforms\n",
    "                prediction = self.lodestar.forward(transforms)\n",
    "                loss_dict = self.lodestar.compute_loss(prediction, inverses)\n",
    "                loss1, loss2 = loss_dict['between_image_disagreement'], loss_dict['within_image_disagreement']\n",
    "                loss = loss1 + loss2\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                self.losses.append(loss.item())\n",
    "            \n",
    "            if self.lower_b != None:\n",
    "                if self.losses[i] < self.lower_b and self.losses[i-1] < self.lower_b and self.losses[i-2] < self.lower_b:\n",
    "                    break\n",
    "\n",
    "    def plot_losses(self, yscale: str='linear') -> None:\n",
    "\n",
    "        \"\"\"Plots losses from training (will not work if you used load_network)\"\"\"\n",
    "\n",
    "        plt.scatter([i for i in list(range(1, len(self.losses)+1))], self.losses, s=1, color='black')\n",
    "        plt.grid()\n",
    "        plt.yscale(yscale)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_scatter(self, image: torch.Tensor, points: list[list[float]], show: bool=False, **kwargs: any) -> None:\n",
    "        \n",
    "        \"\"\"Shows image and points if show=True\"\"\"\n",
    "\n",
    "        plt.imshow(image.detach().numpy().squeeze(), cmap=kwargs.get('cmap'))\n",
    "        for point in points:\n",
    "            plt.scatter(point[0], point[1], marker=kwargs.get('marker'), color=kwargs.get('color'), s=kwargs.get('s', 10))\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def animate(self, dataset: Particle_dataset, points: list[list[float]]=None, show: str=True, **kwargs: any) -> None:\n",
    "\n",
    "        \"\"\"Animates images contained in dataset and scatters points if provided [[points frame 0], [points frame 1] ...]\"\"\"\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(dataset[0].squeeze(), cmap=kwargs.get('cmap'))\n",
    "        if points!=None:\n",
    "            graph = ax.scatter([point[0] for point in points[0]], [point[1] for point in points[0]], \n",
    "                               marker=kwargs.get('marker', 'o'), \n",
    "                               color=kwargs.get('color', 'blue'), \n",
    "                               s=kwargs.get('s'), \n",
    "                               alpha=kwargs.get('alpha')\n",
    "                               )\n",
    "\n",
    "        ax_slider = plt.axes([0.1, 0.01, 0.8, 0.03])\n",
    "        slider = Slider(ax_slider, 'Frame', 0, len(dataset)-1, valinit=0, valstep=1)\n",
    "\n",
    "        def update(val):\n",
    "            frame = int(slider.val)\n",
    "            im.set_array(dataset[frame].squeeze())\n",
    "            if points!=None:\n",
    "                graph.set_offsets(points[frame])\n",
    "            fig.canvas.draw_idle()\n",
    "    \n",
    "        slider.on_changed(update)\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def predict_single(self, image: torch.FloatTensor, plot: bool=True) -> None:\n",
    "\n",
    "        \"\"\"Makes a single prediction of the posisition of one particle.\\n\n",
    "        This function will always predict one particle, and is thus not intended for detection\"\"\"\n",
    "\n",
    "        raw_pred = self.lodestar.pooled(image.unsqueeze(0)).detach().numpy()\n",
    "        processed_pred = raw_pred + np.array([image.shape[2], image.shape[1]]) / 2\n",
    "        position = processed_pred\n",
    "        if plot:\n",
    "            self.plot_scatter(image, position, show=True)\n",
    "        else:\n",
    "            return position\n",
    "\n",
    "    def get_detections(self, image: torch.Tensor, **kwargs) -> np.ndarray:\n",
    "\n",
    "        \"\"\"Returns the positions of all particles detected in the image\"\"\"\n",
    "\n",
    "        raw_preds = self.lodestar.detect(image.unsqueeze(0), cutoff=kwargs.get('cutoff', 0.9), alpha=kwargs.get('alpha', 0.5))[0]\n",
    "        processed_pred = raw_preds + np.array([image.shape[1], image.shape[2]])/2\n",
    "        return processed_pred[:, ::-1]\n",
    "\n",
    "    def detect(self, Input: np.ndarray|torch.Tensor|Particle_dataset, plot: bool=None, **kwargs) -> list[list[float]]|list[list[list[float]]]|None:\n",
    "\n",
    "        \"\"\"Plots/animates all detections in an image/dataset if plot=True,\n",
    "        else it returns the detections\"\"\"\n",
    "        \n",
    "        if 'numpy' in str(type(Input)):\n",
    "            Input = torch.from_numpy(Input)\n",
    "        if 'torch' in str(type(Input)):\n",
    "            image = Input\n",
    "            detections = self.get_detections(image, kwargs.get('cutoff'),kwargs.get('alpha'))\n",
    "            if plot==None or plot:\n",
    "                self.plot_scatter(image, detections)\n",
    "        if 'dataset' in str(type(Input)) or 'Matlab' in str(type(Input)):\n",
    "            dataset = Input\n",
    "            detections = [self.get_detections(dataset[i], **kwargs) for i in range(len(dataset))]\n",
    "            if plot == None or plot:\n",
    "                self.animate(dataset, detections)\n",
    "        else:\n",
    "            print('Input not recognised. Input either an unsqueezed torch tensor, unsqueezed numpy array, or a dataset with unsqueezed images.')\n",
    "        if not plot:\n",
    "            return detections\n",
    "\n",
    "    def get_df(self, dataset: Particle_dataset=None, time_interval: tuple= None, **kwargs: any) -> pd.DataFrame:\n",
    "\n",
    "        \"\"\"Returns a dataframe which can be used in MAGIK\n",
    "        If dataset is not specified, the function uses the trainer's regular data set\"\"\"\n",
    "\n",
    "        data_dict = {'centroid-0': [], 'centroid-1': [], 'frame': [], 'label': [], 'set': [], 'solution': []}\n",
    "        if dataset==None:\n",
    "            dataset=self.dataset\n",
    "        if time_interval==None:\n",
    "            time_interval=[len(dataset)]\n",
    "        for t in range(*time_interval):\n",
    "            detections = self.get_detections(dataset[t], **kwargs)\n",
    "            data_dict['centroid-0'] += [pos[0] for pos in detections]\n",
    "            data_dict['centroid-1'] += [pos[1] for pos in detections]\n",
    "            data_dict['frame'] += [t]*len(detections)\n",
    "            data_dict['label'] += [0]*len(detections)\n",
    "            data_dict['set'] += [0]*len(detections)\n",
    "            data_dict['solution'] += [0]*len(detections)\n",
    "        df = pd.DataFrame.from_dict(data_dict)\n",
    "        self.df = df\n",
    "        return df\n",
    "\n",
    "    def get_score(self, image: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        \"\"\"Returns the detection score map of an image as outlined in the paper\n",
    "        \"Single-shot self-supervised object detection in microscopy\" \\n\n",
    "        The local maxima of the score map are the final detections\"\"\"\n",
    "\n",
    "        yhat = self.lodestar(image.unsqueeze(0))[0]\n",
    "        pred, weight = yhat[:, :-1], yhat[:,-1:]\n",
    "        score = self.lodestar.get_detection_score(pred, weight)\n",
    "        return score\n",
    "    \n",
    "    def show_score(self, image: torch.Tensor, mode: str='3D') -> None:\n",
    "\n",
    "        \"\"\"Shows the detection score map in 3D or in \"2D\" \"\"\"\n",
    "\n",
    "        score = self.get_score(image)\n",
    "\n",
    "        if mode.lower()=='3d':\n",
    "            x = np.arange(0, score.shape[1], 1)\n",
    "            y = np.arange(0, score.shape[0], 1)\n",
    "            X, Y = np.meshgrid(x, y)\n",
    "            Z = score.ravel()\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            ax.plot_surface(X, Y, score, cmap='gray')\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "            ax.set_zlabel('Z')\n",
    "            plt.show()\n",
    "        if mode.lower()=='2d':\n",
    "            plt.imshow(score, cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "    def downscale(self, i: int|list=None) -> None:\n",
    "\n",
    "        \"\"\"Downscales all images in trainer's data set, using maxpool, if indices are not provided\"\"\"\n",
    "\n",
    "        if i==None:\n",
    "            for j in range(len(self.dataset)):\n",
    "                self.dataset[j] = torch.nn.functional.max_pool2d(self.dataset[j], kernel_size=2)\n",
    "        elif type(i)==int:\n",
    "            self.dataset[i] = torch.nn.functional.max_pool2d(self.dataset[i], kernel_size=2)\n",
    "        elif type(i)==list:\n",
    "            for j in i:\n",
    "                self.dataset[j] = torch.nn.functional.max_pool2d(self.dataset[j], kernel_size=2)\n",
    "    \n",
    "    def Normalize(self, value: float, Input: torch.Tensor|Particle_dataset=None) -> None|torch.Tensor|Particle_dataset:\n",
    "\n",
    "        \"\"\"Placeholder function (does not normalize but rescales instead)\"\"\"\n",
    "\n",
    "        if Input==None:\n",
    "            for i in range(len(self.dataset.images)):\n",
    "                self.dataset.images[i] *= value\n",
    "        if 'torch' in str(type(Input)):\n",
    "            Input *= value\n",
    "            return Input\n",
    "        if 'dataset' in str(type(Input)) or 'Matlab' in str(type(Input)):\n",
    "            for i in range(len(Input.images)):\n",
    "                Input.images[i] *= value\n",
    "                return Input\n",
    "    \n",
    "    def load_network(self, path: str) -> None:\n",
    "\n",
    "        \"\"\"Loads network from specified path\"\"\"\n",
    "\n",
    "        self.Network = torch.load(path)\n",
    "        self.lodestar = dl.LodeSTAR(model=self.Network, n_transforms=self.num_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path = r''\n",
    "matlab_files_path = r''\n",
    "dataset = Matlab_dataset(matlab_files_path, Range=(0,10))\n",
    "\n",
    "T = Trainer()\n",
    "T.load_network(network_path)\n",
    "T.set_dataset(dataset)\n",
    "T.Normalize(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Network (Do not run if you loaded a network) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T.add_train_data(T.dataset[0][:,226:290, 426:493]) # 300 nm\n",
    "\n",
    "# T.add_train_data(T.dataset[0][:,295:365,330:399]) # 500 nm Svart\n",
    "# T.add_train_data(T.dataset[0][:,585:654,464:534]) # 500 nm Vit\n",
    "\n",
    "# T.add_train_data(T.dataset[0][:,528:594,147:210]) # 5100 nm svart\n",
    "# T.add_train_data(T.dataset[0][:,95:223, 220:347]) # 5100 nm stor\n",
    "# T.add_train_data(T.dataset[0][:,42:105, 244:306]) # 5100 nm vit\n",
    "\n",
    "T.train(mode='barebones')\n",
    "T.plot_losses(yscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v Required for animations v\n",
    "%matplotlib widget\n",
    "\n",
    "T.detect(dataset, plot=True, fc='None', marker='o')\n",
    "# T.show_score(T.dataset[0])\n",
    "# T.show_data(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export detections to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r''\n",
    "df = T.get_df(T.dataset, cutoff=0.95)\n",
    "df.to_csv(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
